{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div style=\"text-align: center; background: #ff8c00; font-family: 'Montserrat', sans-serif; color: white; padding: 15px; font-size: 30px; font-weight: bold; line-height: 1; border-radius: 20px 20px 0 0; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.2);\">üöÄ Indian Actor recognition using Facenet üöÄ</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">Life Cycle of the Project</div>\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T18:38:11.841396Z","iopub.execute_input":"2023-09-15T18:38:11.841877Z","iopub.status.idle":"2023-09-15T18:38:11.887451Z","shell.execute_reply.started":"2023-09-15T18:38:11.841839Z","shell.execute_reply":"2023-09-15T18:38:11.885495Z"}}},{"cell_type":"markdown","source":"<div style=\"font-size: 14px; font-family: Verdana; border: 2px solid #ccc; background-color: #F5F5F5; padding: 10px; border-radius: 10px; margin-bottom: 20px; position: relative;\">\n\n  <!-- Add the image inside the div, aligned to the right-center -->\n   <img src=\"https://media.giphy.com/media/2lQzj96U6fb5qbknOW/giphy.gif\" style=\"position: absolute; left:40%; right: 0; transform: translateY(-50%); width: 200px; height: auto;\">\n   <img src=\"https://media.giphy.com/media/2lQzj96U6fb5qbknOW/giphy.gif\" style=\"position: absolute; left:60%; right: 0; transform: translateY(-50%); width: 200px; height: auto;\">\n    \n  <span style=\"color: #FF5733; font-weight: bold;\">Steps to be Performed</span>\n  <ol>\n    <li><a href=\"#1\">Loading facenet with pretrained weights</a></li>\n    <li><a href=\"#2\">Loading required packages</a></li>\n    <li><a href=\"#3\">Face Embeddings Genration</a></li>\n    <li><a href=\"#4\">Mapping targets to discrete labels</a></li>\n    <li><a href=\"#5\">Divinding data into train and test splits</a></li>\n    <li><a href=\"#6\">Model Training ü§ñ</a></li>\n    <li><a href=\"#7\">Model Evaluation üìà</a></li>\n    <li><a href=\"#8\">Downloading trained SVM model</a></li>\n    <li><a href=\"#9\">Model Inferencing</a></li>\n    <li><a href=\"#10\">Author Message ‚úâÔ∏è</a></li>\n  </ol>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">Problem Statement</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n\nThe aim of this kernel is to develop a model that is capable of recognizing the trained faces of 135 actors. To accomplish this we are going with the classification of face embeddings generated by Facenet using SVM model.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">1. Building Facenet model Architecture and loading weights.</div>","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Concatenate\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Input\nfrom keras.layers import Lambda\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import add\nfrom keras import backend as K\n\n\ndef scaling(x, scale):\n    return x * scale\n\n\ndef conv2d_bn(x,\n              filters,\n              kernel_size,\n              strides=1,\n              padding='same',\n              activation='relu',\n              use_bias=False,\n              name=None):\n    x = Conv2D(filters,\n               kernel_size,\n               strides=strides,\n               padding=padding,\n               use_bias=use_bias,\n               name=name)(x)\n    if not use_bias:\n        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n        bn_name = _generate_layer_name('BatchNorm', prefix=name)\n        x = BatchNormalization(axis=bn_axis, momentum=0.995, epsilon=0.001,\n                               scale=False, name=bn_name)(x)\n    if activation is not None:\n        ac_name = _generate_layer_name('Activation', prefix=name)\n        x = Activation(activation, name=ac_name)(x)\n    return x\n\n\ndef _generate_layer_name(name, branch_idx=None, prefix=None):\n    if prefix is None:\n        return None\n    if branch_idx is None:\n        return '_'.join((prefix, name))\n    return '_'.join((prefix, 'Branch', str(branch_idx), name))\n\n\ndef _inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n    if block_idx is None:\n        prefix = None\n    else:\n        prefix = '_'.join((block_type, str(block_idx)))\n    name_fmt = partial(_generate_layer_name, prefix=prefix)\n\n    if block_type == 'Block35':\n        branch_0 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_1x1', 0))\n        branch_1 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n        branch_1 = conv2d_bn(branch_1, 32, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n        branch_2 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0c_3x3', 2))\n        branches = [branch_0, branch_1, branch_2]\n    elif block_type == 'Block17':\n        branch_0 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_1x1', 0))\n        branch_1 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n        branch_1 = conv2d_bn(branch_1, 128, [1, 7], name=name_fmt('Conv2d_0b_1x7', 1))\n        branch_1 = conv2d_bn(branch_1, 128, [7, 1], name=name_fmt('Conv2d_0c_7x1', 1))\n        branches = [branch_0, branch_1]\n    elif block_type == 'Block8':\n        branch_0 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_1x1', 0))\n        branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n        branch_1 = conv2d_bn(branch_1, 192, [1, 3], name=name_fmt('Conv2d_0b_1x3', 1))\n        branch_1 = conv2d_bn(branch_1, 192, [3, 1], name=name_fmt('Conv2d_0c_3x1', 1))\n        branches = [branch_0, branch_1]\n    else:\n        raise ValueError('Unknown Inception-ResNet block type. '\n                         'Expects \"Block35\", \"Block17\" or \"Block8\", '\n                         'but got: ' + str(block_type))\n\n    mixed = Concatenate(axis=channel_axis, name=name_fmt('Concatenate'))(branches)\n    up = conv2d_bn(mixed,\n                   K.int_shape(x)[channel_axis],\n                   1,\n                   activation=None,\n                   use_bias=True,\n                   name=name_fmt('Conv2d_1x1'))\n    up = Lambda(scaling,\n                output_shape=K.int_shape(up)[1:],\n                arguments={'scale': scale})(up)\n    x = add([x, up])\n    if activation is not None:\n        x = Activation(activation, name=name_fmt('Activation'))(x)\n    return x\n\n\ndef InceptionResNetV1(input_shape=(160, 160, 3),\n                      classes=128,\n                      dropout_keep_prob=0.8,\n                      weights_path=None):\n    inputs = Input(shape=input_shape)\n    x = conv2d_bn(inputs, 32, 3, strides=2, padding='valid', name='Conv2d_1a_3x3')\n    x = conv2d_bn(x, 32, 3, padding='valid', name='Conv2d_2a_3x3')\n    x = conv2d_bn(x, 64, 3, name='Conv2d_2b_3x3')\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = conv2d_bn(x, 80, 1, padding='valid', name='Conv2d_3b_1x1')\n    x = conv2d_bn(x, 192, 3, padding='valid', name='Conv2d_4a_3x3')\n    x = conv2d_bn(x, 256, 3, strides=2, padding='valid', name='Conv2d_4b_3x3')\n\n    # 5x Block35 (Inception-ResNet-A block):\n    for block_idx in range(1, 6):\n        x = _inception_resnet_block(x,\n                                    scale=0.17,\n                                    block_type='Block35',\n                                    block_idx=block_idx)\n\n    # Mixed 6a (Reduction-A block):\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n    name_fmt = partial(_generate_layer_name, prefix='Mixed_6a')\n    branch_0 = conv2d_bn(x,\n                         384,\n                         3,\n                         strides=2,\n                         padding='valid',\n                         name=name_fmt('Conv2d_1a_3x3', 0))\n    branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n    branch_1 = conv2d_bn(branch_1, 192, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n    branch_1 = conv2d_bn(branch_1,\n                         256,\n                         3,\n                         strides=2,\n                         padding='valid',\n                         name=name_fmt('Conv2d_1a_3x3', 1))\n    branch_pool = MaxPooling2D(3,\n                               strides=2,\n                               padding='valid',\n                               name=name_fmt('MaxPool_1a_3x3', 2))(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=channel_axis, name='Mixed_6a')(branches)\n\n    # 10x Block17 (Inception-ResNet-B block):\n    for block_idx in range(1, 11):\n        x = _inception_resnet_block(x,\n                                    scale=0.1,\n                                    block_type='Block17',\n                                    block_idx=block_idx)\n\n    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n    name_fmt = partial(_generate_layer_name, prefix='Mixed_7a')\n    branch_0 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 0))\n    branch_0 = conv2d_bn(branch_0,\n                         384,\n                         3,\n                         strides=2,\n                         padding='valid',\n                         name=name_fmt('Conv2d_1a_3x3', 0))\n    branch_1 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n    branch_1 = conv2d_bn(branch_1,\n                         256,\n                         3,\n                         strides=2,\n                         padding='valid',\n                         name=name_fmt('Conv2d_1a_3x3', 1))\n    branch_2 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n    branch_2 = conv2d_bn(branch_2, 256, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n    branch_2 = conv2d_bn(branch_2,\n                         256,\n                         3,\n                         strides=2,\n                         padding='valid',\n                         name=name_fmt('Conv2d_1a_3x3', 2))\n    branch_pool = MaxPooling2D(3,\n                               strides=2,\n                               padding='valid',\n                               name=name_fmt('MaxPool_1a_3x3', 3))(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=channel_axis, name='Mixed_7a')(branches)\n\n    # 5x Block8 (Inception-ResNet-C block):\n    for block_idx in range(1, 6):\n        x = _inception_resnet_block(x,\n                                    scale=0.2,\n                                    block_type='Block8',\n                                    block_idx=block_idx)\n    x = _inception_resnet_block(x,\n                                scale=1.,\n                                activation=None,\n                                block_type='Block8',\n                                block_idx=6)\n\n    # Classification block\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - dropout_keep_prob, name='Dropout')(x)\n    # Bottleneck\n    x = Dense(classes, use_bias=False, name='Bottleneck')(x)\n    bn_name = _generate_layer_name('BatchNorm', prefix='Bottleneck')\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False,\n                           name=bn_name)(x)\n\n    # Create model\n    model = Model(inputs, x, name='inception_resnet_v1')\n    if weights_path is not None:\n        model.load_weights(weights_path)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-15T19:54:21.935623Z","iopub.execute_input":"2023-09-15T19:54:21.936185Z","iopub.status.idle":"2023-09-15T19:54:21.986523Z","shell.execute_reply.started":"2023-09-15T19:54:21.936140Z","shell.execute_reply":"2023-09-15T19:54:21.984629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing the model.\nembeddings_generator = InceptionResNetV1(\n        input_shape=(None, None, 3),\n        classes=128,\n    )\n# Loading the prebuilt weights.\nembeddings_generator.load_weights('/kaggle/input/indian-actor-faces-for-face-recognition/facenet_keras_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T19:54:21.988777Z","iopub.execute_input":"2023-09-15T19:54:21.989176Z","iopub.status.idle":"2023-09-15T19:54:27.653523Z","shell.execute_reply.started":"2023-09-15T19:54:21.989141Z","shell.execute_reply":"2023-09-15T19:54:27.651673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">2. Loading Required packages.</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.preprocessing import LabelEncoder,Normalizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import FileLink\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:22:06.357817Z","iopub.execute_input":"2023-09-15T20:22:06.358312Z","iopub.status.idle":"2023-09-15T20:22:06.366446Z","shell.execute_reply.started":"2023-09-15T20:22:06.358277Z","shell.execute_reply":"2023-09-15T20:22:06.365210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n<h3 align=\"left\"><font color='#DEB887'>üí° About Packages: </font></h3>\n    \n* matplotlib              - To display plots and images inline in the notebook.\n* seaborn                 - Fore creating visually appealing plots\n* numpy                   - For numerical computations.\n* pandas                  - To perform operations on tabular data.\n* cv2                     - For operations realted to images.\n* os                      - To access local file directory.\n* tqdm                    - To display the progress of loops.\n* sklearn.metrics         - For evaluating the performance of the model\n* sklearn.preprocessing   - For functions that helps in preprocessing the data.\n* sklearn.svm             - For making use of Support Vector machine algorithm.\n* sklearn.model_selection - For dividing the dataset into train and test\n* Ipython.display         - For displaying the links using which the utils can be donwloaded.\n* joblib                  - To save models and pickle pipelines for restoring them in deployment environment.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">3. Face Embeddings Generation</div>","metadata":{}},{"cell_type":"code","source":"def load_images_and_labels(dir_path,model):\n    \"\"\"\n    Parameters\n    ----------\n    dir_path : base directory path that contains the folders of each actor.\n    model : facenet model that is loaded with trained weights. (Transfer learning)\n    \n    Functionality\n    -------------\n    1. Function reads all the images in the base directory by traversing recursively across all folders.\n    2. All the imagees are standardized and the 128 face embeddings will be generated for all the images.\n    \n    Output\n    ------\n    face_pixels : list of numpy arrays where each numpy array represents an image (face).\n    face_embeddings : list of embeddings for all the images read.\n    labels : list of labels where each label represents the name of the actor\n    \"\"\"\n    face_pixels,face_embeddings,labels=[],[],[]\n    for actor_dir in tqdm(os.listdir(dir_path)):\n        for actor_image in os.listdir(dir_path+actor_dir):\n            # reading the image\n            img=cv2.imread(dir_path+actor_dir+'/'+actor_image)\n            \n            # converting the image to RGB format\n            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n            \n            # Resizing the image to the size required by Facenet\n            img=cv2.resize(img,(160,160))\n            img = img.astype('float32')\n            \n            # Normalizing the images\n            mean, std = img.mean(), img.std()\n            img = (img - mean) / std\n            face_pixels.append(img)\n            labels.append(actor_dir)\n            img = np.expand_dims(img, axis=0)\n            \n            # Getting the face embeddings from the model.\n            face_embeddings.append(model.predict(img))\n    return face_pixels,face_embeddings,labels","metadata":{"execution":{"iopub.status.busy":"2023-09-15T19:54:29.338157Z","iopub.execute_input":"2023-09-15T19:54:29.338628Z","iopub.status.idle":"2023-09-15T19:54:29.350923Z","shell.execute_reply.started":"2023-09-15T19:54:29.338595Z","shell.execute_reply":"2023-09-15T19:54:29.349385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir='/kaggle/input/indian-actor-faces-for-face-recognition/actors_dataset/Indian_actors_faces/'\nface_pixels,face_embeddings,labels=load_images_and_labels(base_dir,embeddings_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the lists into numpy arrays\nface_embeddings=np.array(face_embeddings)\nface_pixels=np.array(face_pixels)\nlabels=np.array(labels)\nface_embeddings=face_embeddings.reshape(face_embeddings.shape[0],face_embeddings.shape[2])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:12:57.277628Z","iopub.execute_input":"2023-09-15T20:12:57.278156Z","iopub.status.idle":"2023-09-15T20:12:59.909165Z","shell.execute_reply.started":"2023-09-15T20:12:57.278108Z","shell.execute_reply":"2023-09-15T20:12:59.907506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#006600; font-size:140%; text-align:left;padding: 0px; border-bottom: 3px solid #003300\"> Why is it required to store embeddings?</p>\nFacenet takes time for generating embeddings for each images. It takes around 15 miutes to generate face embeddings for the entire dataset. This is a computation intensive task and hence it is always suggested to store the results and reuse them whenever required rather than generating them again.","metadata":{}},{"cell_type":"code","source":"# saving these files to overcome processing time of the dataset\nnp.savez_compressed('/kaggle/working/data.npz',a=face_embeddings,b=face_pixels,c=labels)\nFileLink(r'data.npz')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:12:59.910913Z","iopub.execute_input":"2023-09-15T20:12:59.912178Z","iopub.status.idle":"2023-09-15T20:15:02.235920Z","shell.execute_reply.started":"2023-09-15T20:12:59.912133Z","shell.execute_reply":"2023-09-15T20:15:02.234716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading back the generated embeddings, face images and labels\ndata=np.load('/kaggle/working/data.npz')\nface_embeddings,face_pixels,labels=data['a'],data['b'],data['c']\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:15:02.237446Z","iopub.execute_input":"2023-09-15T20:15:02.237832Z","iopub.status.idle":"2023-09-15T20:15:12.829937Z","shell.execute_reply.started":"2023-09-15T20:15:02.237798Z","shell.execute_reply":"2023-09-15T20:15:12.828027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#006600; font-size:140%; text-align:left;padding: 0px; border-bottom: 3px solid #003300\">Checking the authenticity of Embeddings</p>\nA small check to make sure that the embeddings generated are proper or not. Found few methods over web that are generating the redundant embeddings for the list of images. They are generating the same embeddings for multiple images. So, we will add a check where we look for unique embeddings. The embeddings of same face will also differ a bit. If we find instances of redundant embeddings, we should make sure that it is corrected before proceeding further.","metadata":{}},{"cell_type":"code","source":"# check the authenticity of the generated emebeddings\ntemp=pd.DataFrame(face_embeddings)\nprint(\"Total embeddings generated: \",len(temp))\nprint(\"Total embeddings after dropping duplicates: \",len(temp.drop_duplicates()))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:13.386145Z","iopub.execute_input":"2023-09-15T20:17:13.386655Z","iopub.status.idle":"2023-09-15T20:17:13.499836Z","shell.execute_reply.started":"2023-09-15T20:17:13.386619Z","shell.execute_reply":"2023-09-15T20:17:13.498647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the number of embeddings did not change even after dropping duplicates, we are moving in correct direction and embeddings generated are valid.","metadata":{}},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">4. Conversion of names of actor faces to discrete labels</div>","metadata":{}},{"cell_type":"code","source":"# Fitting a label encoder of names of actors\nlabel_encoder=LabelEncoder()\nencoded_labels=label_encoder.fit_transform(labels)\n\n# storing the label encoder object for using in deplyment environment\njoblib.dump(label_encoder, 'label_encoder.sav')\n\n# Downloading the picked label encoder file\nFileLink(r'label_encoder.sav')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:18.075812Z","iopub.execute_input":"2023-09-15T20:17:18.076535Z","iopub.status.idle":"2023-09-15T20:17:18.090417Z","shell.execute_reply.started":"2023-09-15T20:17:18.076497Z","shell.execute_reply":"2023-09-15T20:17:18.088875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">5. Splittig the data into train and test sets</div>","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(face_embeddings,encoded_labels,test_size=0.3,stratify=encoded_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:21.112245Z","iopub.execute_input":"2023-09-15T20:17:21.112691Z","iopub.status.idle":"2023-09-15T20:17:21.130084Z","shell.execute_reply.started":"2023-09-15T20:17:21.112657Z","shell.execute_reply":"2023-09-15T20:17:21.128649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of Xtrain : {x_train.shape}\")\nprint(f\"Shape of Ytrain : {y_train.shape}\")\nprint(f\"Shape of Xtest : {x_test.shape}\")\nprint(f\"Shape of Ytest : {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:23.882495Z","iopub.execute_input":"2023-09-15T20:17:23.883255Z","iopub.status.idle":"2023-09-15T20:17:23.891319Z","shell.execute_reply.started":"2023-09-15T20:17:23.883206Z","shell.execute_reply":"2023-09-15T20:17:23.889946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">6. Training a SVM model on data</div>","metadata":{}},{"cell_type":"code","source":"model = SVC(probability=True)\nmodel.fit(x_train,y_train )","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:27.227504Z","iopub.execute_input":"2023-09-15T20:17:27.228466Z","iopub.status.idle":"2023-09-15T20:17:39.901201Z","shell.execute_reply.started":"2023-09-15T20:17:27.228412Z","shell.execute_reply":"2023-09-15T20:17:39.899935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat_train = model.predict(x_train)\nyhat_test = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:17:40.142583Z","iopub.execute_input":"2023-09-15T20:17:40.143091Z","iopub.status.idle":"2023-09-15T20:17:51.213133Z","shell.execute_reply.started":"2023-09-15T20:17:40.143054Z","shell.execute_reply":"2023-09-15T20:17:51.211771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">7. Model Evaluation</div>","metadata":{}},{"cell_type":"markdown","source":"> <span style='font-size:15px; font-family:Verdana;color: #FF00CC;'><b>7.1 |Accuracy Score</b></span>","metadata":{}},{"cell_type":"code","source":"# calculating accuracy score on train set\nscore_train = accuracy_score(y_train, yhat_train)\n\n# calculating accuracy score on test set\nscore_test = accuracy_score(y_test, yhat_test)\n\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:18:11.278927Z","iopub.execute_input":"2023-09-15T20:18:11.279429Z","iopub.status.idle":"2023-09-15T20:18:11.291547Z","shell.execute_reply.started":"2023-09-15T20:18:11.279392Z","shell.execute_reply":"2023-09-15T20:18:11.289999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style='font-size:15px; font-family:Verdana;color: #FF00CC;'><b>7.2 | Finding the probability for the prediction</b></span>","metadata":{}},{"cell_type":"code","source":"# Finding out probabilites of first test sample for every target class\nprobs_svc = model.decision_function([x_test[0]])\nprobs_svc = (probs_svc - probs_svc.min()) / (probs_svc.max() - probs_svc.min())\nprobs_svc","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:19:09.936802Z","iopub.execute_input":"2023-09-15T20:19:09.937243Z","iopub.status.idle":"2023-09-15T20:19:10.230556Z","shell.execute_reply.started":"2023-09-15T20:19:09.937208Z","shell.execute_reply":"2023-09-15T20:19:10.229073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style='font-size:15px; font-family:Verdana;color: #FF00CC;'><b>7.3 | Classification Report</b></span>","metadata":{}},{"cell_type":"code","source":"print(classification_report(yhat_test,y_test,target_names=label_encoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:21:32.141050Z","iopub.execute_input":"2023-09-15T20:21:32.141533Z","iopub.status.idle":"2023-09-15T20:21:32.168341Z","shell.execute_reply.started":"2023-09-15T20:21:32.141496Z","shell.execute_reply":"2023-09-15T20:21:32.167435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style='font-size:15px; font-family:Verdana;color: #FF00CC;'><b>7.4 | Confusion Matrix</b></span>","metadata":{}},{"cell_type":"code","source":"print(confusion_matrix(yhat_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:24:43.896721Z","iopub.execute_input":"2023-09-15T20:24:43.897361Z","iopub.status.idle":"2023-09-15T20:24:43.907393Z","shell.execute_reply.started":"2023-09-15T20:24:43.897309Z","shell.execute_reply":"2023-09-15T20:24:43.906519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">8. Downloaing the trained SVM model.</div>\n","metadata":{}},{"cell_type":"code","source":"joblib.dump(model, 'svm_classifier.sav')\nFileLink(r'svm_classifier.sav')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:22:58.863908Z","iopub.execute_input":"2023-09-15T20:22:58.864836Z","iopub.status.idle":"2023-09-15T20:22:58.900218Z","shell.execute_reply.started":"2023-09-15T20:22:58.864794Z","shell.execute_reply":"2023-09-15T20:22:58.898764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<div style=\"text-align: center; background: #1ED760; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">9. Model Inferencing.</div>","metadata":{}},{"cell_type":"markdown","source":"You can test the model with any image in the dataset. To test the model on unseen image, we need to deploy it which will be done later. Now, everytime you execute the below cell, you will get a new image displayed. The title of the image contains the actual name of the actor and the X label of the image contains the predicted name of the actor in the image.","metadata":{}},{"cell_type":"code","source":"random_index=np.random.choice(len(labels))\npredicted_label=label_encoder.inverse_transform(model.predict([face_embeddings[random_index]]))[0]\nactual_label=labels[random_index]\nplt.title(f\"Actual Label: {actual_label} \")\nplt.xlabel(f\"Predicted Label: {predicted_label}\")\nplt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\nplt.imshow(face_pixels[random_index])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:38:31.408837Z","iopub.execute_input":"2023-09-15T20:38:31.409329Z","iopub.status.idle":"2023-09-15T20:38:31.833620Z","shell.execute_reply.started":"2023-09-15T20:38:31.409292Z","shell.execute_reply":"2023-09-15T20:38:31.831780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; background:  #FF00CC; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">Saving one embedding for each unique face in the dataset</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n<h3 align=\"left\"><font color='#DEB887'>üí° Why to store one embedding for every unique face in the dataset?</font></h3>   \nThis would help in the process of inferencing. After the generation of embeddings, we can recognize the face using either a trained model or using some distances. The embeddings will be in such a way that the distance between two embeddings of the same face will have very less distance when compare to embeddings of different face. This core logic can also be used while inferencing.\n</div>","metadata":{}},{"cell_type":"code","source":"# Saving one embedding for each face\nmapping_dict = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nknown_face_embeddings=[]\nfor key in mapping_dict:\n    match_indices=np.where(labels==key)\n    first_match_index=match_indices[0][0]\n    known_face_embeddings.append(face_embeddings[first_match_index])\nknown_face_embeddings=np.array(known_face_embeddings)\nnp.savez_compressed('/kaggle/working/data.npz',a=known_face_embeddings)\nFileLink(r'data.npz')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T20:40:56.524556Z","iopub.execute_input":"2023-09-15T20:40:56.525976Z","iopub.status.idle":"2023-09-15T20:40:56.648348Z","shell.execute_reply.started":"2023-09-15T20:40:56.525925Z","shell.execute_reply":"2023-09-15T20:40:56.646572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n<div style=\"text-align: center; background:  #FF00CC; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 15px; font-size: 26px; font-weight: bold; line-height: 1; border-radius: 50% 0 50% 0 / 40px; margin-bottom: 20px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\">10. Author Message</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:white;font-size:15px;font-family:Georgia;border-style: solid;border-color: #FF00CC;border-width:3px;padding:10px;margin: 1px;color:#254E58;overflow:hidden\"> \n\n<center><h4><b style = 'color: red;'>Author :</b> Nagasai Biginepalli </h4> </center>\n<hr></hr>\n<center><b><u>Networking Links</u></b></center>\n<b>üëâRead more project :</b> https://www.kaggle.com/nagasai524 <br>\n<b>üëâShoot me mails :</b> www.biginepallinagasai109@gmail.com<br>\n<b>üëâConnect on LinkedIn :</b> https://www.linkedin.com/in/nagasai-biginepalli-64648a146/<br>\n<b>üëâExplore Github :</b> https://github.com/Nagasai524 <br>\n    \n<hr>\n    \n<center> <strong> If you liked this Notebook, please do upvote. </strong>\n    \n<center> <strong> If you have any questions, feel free to comment! </strong>\n    \n<center> <strong style = 'color: red;' > ‚ú®Best Wishes‚ú® </strong>","metadata":{}},{"cell_type":"markdown","source":"<center> <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >‚¨ÜÔ∏èBack to the Top ‚¨ÜÔ∏è</a>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}